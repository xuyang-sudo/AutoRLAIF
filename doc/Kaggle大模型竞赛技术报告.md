# 数据集描述

比赛数据集由ChatBot Arena的用户交互组成。在每次用户交互中，评判向两个不同的大型语言模型提供一个或多个提示，然后指出哪个模型给出了更令人满意的回应。比赛的目标是预测评判的偏好，并确定给定的提示/回应对被选为获胜者的可能性。

ChatBot Arena 网址：https://arena.lmsys.org/
![image](![image](image/image.png))
**注意**：这是一个代码竞赛。当您的提交被评分时，这个示例测试数据将被完整的测试集替换。训练数据中有55K行，测试集中预计有大约25,000行。

## 文件

### train.csv

- `id` - 行的唯一标识符。
- `model_[a/b]` - model_[a/b]的身份。包含在train.csv中但不在test.csv中。
- `prompt` - 作为输入给出的提示（给两个模型）。
- `response_[a/b]` - model_[a/b]对给定提示的回应。
- `winner_model_[a/b/tie]` - 标记评判选择的二进制列。真实目标列。

### test.csv

- `id`
- `prompt`
- `response_[a/b]`

### sample_submission.csv

一个正确格式的提交文件。

- `id`
- `winner_model_[a/b/tie]` - 这是从测试集预测的内容。

**注意**：此竞赛的数据集包含可能被认为是亵渎、粗俗或冒犯的文本。

# 任务描述

本竞赛要求参赛者基于 `train.csv` 数据集进行模型训练，并对未公开的 `test.csv` 数据集进行预测。预测目标包含三个类别：

- `winner_model_a`: 模型 a 的回答更符合人类偏好
- `winner_model_b`: 模型 b 的回答更符合人类偏好
- `winner_tie`: 两个模型的回答同等优秀或同等不佳

预测结果应为每个类别的概率，且三个概率之和必须等于 1。

# 评估指标

本竞赛采用对数损失（Log Loss）作为评估指标。

## Log Loss

对数损失是`分类`问题中常用的评估指标，适用于需要`输出概率`的多类别分类任务。它衡量的是预测`概率分布`与真实`标签分布`之间的差异。

### 计算公式

对于单个样本：

```
LogLoss = -∑(y_i * log(p_i))
```

其中：
- y_i 是真实标签的one-hot编码
- p_i 是预测的概率

### 特点

`惩罚性强`：错误预测会受到严厉惩罚，尤其是`高置信度`的`错误预测`。

数学描述：

考虑二分类问题，真实标签 y ∈ {0, 1}，预测概率为 p。

对于正确预测（y = 1 时 p → 1，或 y = 0 时 p → 0）：
lim(p→1) -log(p) = 0
lim(p→0) -log(1-p) = 0

对于错误预测（y = 1 时 p → 0，或 y = 0 时 p → 1）：
lim(p→0) -log(p) = +∞
lim(p→1) -log(1-p) = +∞

关键不对称性：
对于 ε → 0:
-log(1-ε) ≈ ε （正确高置信预测的奖励）
-log(ε) ≈ -log(ε) （错误高置信预测的惩罚）

由于 -log(ε) >> ε 当 ε → 0，

**这表明对高置信度错误预测的惩罚远大于对高置信度正确预测的奖励。**

![](D:\xgswj\求职\简历-2024-10-24\image\image-1.png)

以下是对对话数据表示的补充说明，并用表格展示原数据格式和拼接后格式：

# 对话数据表示

原数据中给出的格式为 prompt: ["prompt_1", "prompt_2", ···], response_a和response_b也是一样。首先需要将其解析为JSON格式，并将其中的字符串部分提取出来，拼接成需要的样子：

- prompt: prompt_1 + prompt_2 + ···
- response_a: response_a_1 + response_a_2 + ···
- response_b: response_b_1 + response_b_2 + ···

在拼接过程中，我注意到以下几点：
1. 保持原有的对话顺序
2. 在不同的prompt或response之间添加适当的分隔符（如换行符）让模型得以区分

| 数据字段 | 原数据格式 | 拼接后格式 |
|---------|-----------|-----------|
| prompt  | ["你好，请问今天天气如何？", "谢谢，那明天呢？"] | "你好，请问今天天气如何？谢谢，那明天呢？" |
| response_a | ["今天天气晴朗，气温适宜。", "根据预报，明天多云，有小雨。"] | "今天天气晴朗，气温适宜。根据预报，明天多云，有小雨。" |
| response_b | ["今日阳光明媚，温度宜人。", "预计明天阴天，可能有零星小雨。"] | "今日阳光明媚，温度宜人。预计明天阴天，可能有零星小雨。" |

这里可以看到这种处理方式丢失了一部分信息，`多轮对话`的`轮次`没有被表现出来，所以更理想的表示方式是：
`round_n : <prompt>: prompt_n + <response_a>: response_a_n + <response_b>: response_b_n`

| 数据字段 | 原数据格式 | 改进后的格式 |
|---------|-----------|-------------|
| prompt  | ["你好，请问今天天气如何？", "谢谢，那明天呢？"] | round_1: <prompt>: 你好，请问今天天气如何？<br>round_2: <prompt>: 谢谢，那明天呢？ |
| response_a | ["今天天气晴朗，气温适宜。", "根据预报，明天多云，有小雨。"] | round_1: <response_a>: 今天天气晴朗，气温适宜。<br>round_2: <response_a>: 根据预报，明天多云，有小雨。 |
| response_b | ["今日阳光明媚，温度宜人。", "预计明天阴天，可能有零星小雨。"] | round_1: <response_b>: 今日阳光明媚，温度宜人。<br>round_2: <response_b>: 预计明天阴天，可能有零星小雨。 |

完整对话示例：

```
round_1: 
<prompt>: 你好，请问今天天气如何？
<response_a>: 今天天气晴朗，气温适宜。
<response_b>: 今日阳光明媚，温度宜人。

round_2:
<prompt>: 谢谢，那明天呢？
<response_a>: 根据预报，明天多云，有小雨。
<response_b>: 预计明天阴天，可能有零星小雨。
```

这种表示方式的优点是`保持了对话的时序性，有助于理解上下文关系`。

# EDA 数据探索性分析

## prompt, response的tokens数量分析

![image-1](D:\xgswj\求职\简历-2024-10-24\image\image-2.png)

根据可视化分析，取`3072`为最大序列长度，取该数字是因为其为16的倍数，在GPU动态计算图中，所有的向量都要被padding为4、8、16的倍数才能被GPU批量计算，如果不提前设定为16的倍数，意味着GPU运行时需要动态处理每一个数据的padding，导致超时问题。

另外，`response_a`和`response_b`的长度分布几乎一致
![image-1](D:\xgswj\求职\简历-2024-10-24\image\image-3.png)

# 模型选择
本场比赛可用的模型非常多，最终选择了`Gemma-2-9b-it`。主要原因是llama系列模型（包括llama3.1）未在部分数据上进行预训练并且参数调整较为复杂，而Gemma-2的后训练(`post-training`)阶段使用了主办方的ChatBot-1M数据集，这意味着Gemma-2在该领域任务已经具有有一定的适应能力，实验也表明Gemma-2的表现显著比其他模型更好。
`it`含义是该模型已经经过指令对齐，掌握了根据人类指令生成回复的能力。

## LLM的两阶段训练
补充介绍一下大型语言模型(LLM)训练的两个主要阶段:预训练(Pre-training)和微调(Fine-tuning)。

1. 预训练阶段 (Pre-training):

预训练是LLM训练的第一个也是最基础的阶段。在这个阶段中:

- 目标: 让模型学习语言的基本结构、语法规则和广泛的知识。
- 数据: 使用海量的无标注文本数据，通常包括网页、书籍、文章等多种来源。
- 方法: 常用的是自监督学习，如掩码语言模型(MLM)或因果语言模型(CLM)。
- 特点: 
  - 计算密集型，通常需要大量的GPU资源和时间，如在5000张H100的集群训练超过1个月时间。
  - 模型在这个阶段学习到的是通用的语言理解能力。
  - 预训练后的模型可以作为多种下游任务的基础。

2. 微调阶段 (Fine-tuning):

微调是在预训练模型基础上进行的针对性训练:

- 目标: 让模型适应特定的任务或领域。
- 数据: 使用与目标任务相关的标注数据集。
- 方法: 有监督学习，模型根据特定任务的输入输出对来调整参数。
- 特点:
  - 相比预训练，所需资源较少。
  - 可以显著提高模型在特定任务上的表现。
  - 可能会导致模型在其他任务上的性能下降（灾难性遗忘）。

在Gemma-2的案例中，后训练(Post-training)可以看作是预训练和微调之间的一个中间步骤，或者一个特殊的微调过程。它使用了更加专业或特定领域的大规模数据集（如ChatBot-1M），但训练方式更接近预训练。这个步骤的目的是在保持模型通用性的同时，增强其满足人类偏好的能力。

Gemma-2技术报告：https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf

# 微调方法选择 - QLoRA

## 什么是QLoRA

QLoRA是一种高效的模型微调技术，它结合了量化（Quantization）和低秩适应（LoRA）方法。这种方法允许在`有限的计算资源`下微调大型语言模型，同时保持`接近全参数微调`的性能。

- 使用4位量化来压缩主模型参数
- 仅训练低秩适应矩阵 `adapter`
![image-1](D:\xgswj\求职\简历-2024-10-24\image\image-4.png)

## 数学原理

QLoRA的核心原理包括：

1. 量化：
   将32位浮点参数压缩为4位整数，大幅减少内存使用。
   
   数学表示：Q(x) = round(x / scale) * scale

2. 低秩适应（LoRA）：
   对于权重矩阵W，LoRA添加一个低秩更新：
   
   W' = W + BA
   
   其中B∈R^(d×r)，A∈R^(r×k)，r是秩（通常远小于d和k）

3. 反向传播：
   在训练过程中，梯度仅通过LoRA参数（A和B）传播，原始模型参数保持冻结。

## 关键参数和影响

1. 量化位数：影响模型精度。
2. LoRA秩（r）：较大的r提供更多适应能力，但增加参数量。
3. alpha：影响训练稳定性和收敛速度, 作用等同于学习率。
4. 适应层选择：决定哪些层应用LoRA，影响模型适应能力。（freeze）

## 初始化LoRA参数：PISSA

PISSA（Parameter-Efficient Initialization for Sequentially Stacked Adaptation）是一种接近全参数微调性能的初始化方法：

1. 原理：利用预训练模型的参数分布来初始化LoRA权重。
2. 方法：
   - 对预训练权重进行SVD分解
   - 使用分解得到的矩阵初始化LoRA的A和B
3. 优势：
   - 提供更好的起始点，加速收敛
   - 改善最终性能
   - 数据量越少的场景，相对其他方法效果越好

4. 实现：
   ```python
   U, S, Vh = torch.svd(weight)
   lora_A = U[:, :rank] * torch.sqrt(S[:rank])
   lora_B = torch.sqrt(S[:rank]).unsqueeze(0) * Vh[:rank, :]
   ```

LoRA结合PISSA初始化可以在资源受限的情况下实现接近全参数微调的效果。

PISSA的gitgub：https://github.com/GraphPKU/PiSSA
PISSA的论文：https://arxiv.org/pdf/2404.02948.pdf

# 训练和迭代过程
总共经历五轮迭代：
1. 对于额外数据集的利用
本场比赛中，公布的额外数据集有两个，一个是157k的由GPT-4打分得到的合成数据集，另一份是主办方2023年发布的33k对话数据。一般来说额外的数据集总会有distribution shift（分布偏移）问题，往往只能反映一部分训练集的特征分布，所以一般会在额外数据集上进行第一阶段的训练，再在比赛数据上进行第二阶段的训练。本次我使用的策略是直接混合训练，实际上是一个经历过数据洞察的选择。

当判断新数据是否存在分布偏移问题时，最准确的办法就是，使用在比赛数据上训练好的模型，在额外数据集上进行预测并计算其指标是否接近比赛训练数据。

通过这种方法我检测到157k这份由GPT-4进行标注的数据与真实人类的标注是有巨大的分布偏移的，尽管其原论文表示GPT-4的打分与人类的一致性达到84%，甚至超越人类和人类之间的一致性，这其实是由于构建该数据集的prompt都为专业评估LLM性能指标的问题，比如常识或代码问题，是有标准答案的。但是在真实世界的用户提问上，LLM的评估标准和人类有巨大的差异，比如人类对信息的真实性和正确性没有那么强的判断能力，而LLM可以。

而另一份33k数据则在这种洞察下表现非常一致，谨慎起见，我使用交叉验证方式来确保
两者分布一致：首先使用比赛数据训练，并在33k数据集上评估；然后使用33k数据训练，在比赛数据上评估。两次实验都表现出很高的一致性，表明两数据集几乎一致——这是非常难得的，基本上很难有两份数据集的分布能保持一致其实。所以我大胆将两者数据直接混合。

1.1. 确保训练过程中模型所见标签比例一致

想象一个极端的场景：如果三个类别以这样的顺序出现在训练数据：000000000,11111111,22222222，那么模型会学到什么？什么都学不到，模型在这个过程中先是学到了全部预测0，然后是全部预测1，然后是全部预测2，最终只会全部预测2. 为了确保模型不会产生这样的偏见，我需要在训练过程中迭代的数据上做处理，保证任何一个按顺序截取的数据子集内的类别比例是相同的。

数学描述：
设训练集 D = {(x_i, y_i)}_{i=1}^N，其中 y_i ∈ {0, 1, 2} 为类别标签。
对于任意长度为 L 的连续子序列 S_L ⊂ D，我们希望：

P(y = k | x ∈ S_L) ≈ P(y = k | x ∈ D), ∀k ∈ {0, 1, 2}


实现示例（使用PyTorch）：

```python
from torch.utils.data import DataLoader, WeightedRandomSampler
import numpy as np

def create_balanced_sampler(dataset):
    class_counts = np.bincount(dataset.targets)
    class_weights = 1. / class_counts
    weights = class_weights[dataset.targets]
    sampler = WeightedRandomSampler(weights, len(weights))
    return sampler

# 使用
train_sampler = create_balanced_sampler(train_dataset)
train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)
```
#####################################################################
该方案使得单模 lb 达到0.922，当时处于70名左右

2. TTA (测试时增强)
根据对LLM偏好的论文研究，所有的LLM（包括GPT-4）都对先出现的第一个response更偏好，这导致天然的类别预测不平衡问题，为了抵消对于第一个response的偏好，自然想到了让a, b两者的response轮流做`第一个response`。具体做法就是在`infer`时交换a和b的顺序，分别推理一遍并取平均。
##################################################################
TTA使得 lb 达到0.912, 到达当时的50名左右

3. max_tokens=3072
正常来说，训练时的max_tokens不会开到超过1536以上，因为消耗的时间和显存都是随着max_tokens指数增长的，开太大会导致一轮实验时间非常久。我对max_tokens参数做了消融实验，得出3072是相对最优参数。
##################################################################
3072使得 lb 达到了 0.903，到达当时的30名左右

4. LoRA Rank = 64
为了使QLoRA微调的效果能接近全参数微调，在 **4卡A800** 上使用`DeepSpeed`分布式训练框架，做了该参数的消融实验，得出结论：在LoRA Rank超过64以后性能几乎停止增长，同时耗时以指数级增长，所以最适合继续迭代的参数就为64，同时也有助于线上推理在9h内完成。
##################################################################
r = 64 使得 lb 达到 0.896，达到当时的21名

5. 替换score的单层linear为三层linear

做法简单，但是motivation是比较深刻的。
从两个角度解释
1. 原模型最后一层在两阶段训练过程中，所做的任务都是`预测下一个token`，这意味着`lm head`层前的整个层所做的事情基本上是`表征要预测的token是什么`，而这与我所需的`分类任务`是不匹配的。
解决这一不匹配问题有两种思路：SFT，替换score层

首先尝试的是 SFT ，限制模型输出 a, b, tie三个token, 并在计算损失时只计算这三个token的概率的logloss。表面上看这一方法契合原有的预训练方法，但导致了新的问题：模型并不倾向于直接输出答案，而是首先输出分析过程，比如 The response a is about ... 这是由于所有的模型在后训练阶段都被 `思维链`数据微调过来增强其推理能力。并且只约束三个token的输出概率分布，会导致剩下的10w+token的概率分布得不到约束从而产生混乱，干扰模型学习过程。

于是，我尝试了替换 score 层，如果说让LoRA + 一层分类层去做新任务领域适应很困难，那干脆不要改变最后一层原本的任务偏好，而是训练新的层去适配原来这层的偏好，根据原来层的偏好，转换为相应的三分类概率分布输出，这就降低了原模型调整任务的难度。

2. 数学等价≠优化路径相同 （你的数据量和GPU资源不是无穷的）
对于 score 层只简单的由一层Linear层加到了3层，并且没有加入激活函数，这意味着数学上，前后两个score层是等价的，理论上两者效果是相等的。但是，最终效果相当的前提是：`无穷数据量`和`无限计算资源`下成立的，而实际上，`有限的数据`，`有限的计算资源`会限制神经网络，使其达不到最终状态。

一般来说，数据叫`输入`，把概率分布叫`输出`，这是在`前向传播`的叫法。反过来，对`反向传播`而言，`目标的概率分布`反而成为输入了。这意味着反向传播过程中，神经网络实际上也承担了`表征目标概率分布和预测概率分布的差`的表示任务，而更大的score层对于该表示有更强的鲁棒性，因为神经元更多，捕获的特征数量更多，在weight decay的约束下，单个特征的重要性又下降了，使得反向传播的梯度表示更鲁棒，从而优化了整个优化路径。

# Things didn't work

LESS 数据筛选方法 --> 导致过拟合 / 置信度过高导致logloss惩罚严重
论文：https://arxiv.org/abs/2402.04333
github: https://arxiv.org/abs/2402.04333

EMO loss --> 用于 SFT 可以有效缓解置信度过高问题，使得loss计算更合理，然而无法解决其他token的概率分布约束问题
解读：https://kexue.fm/archives/9797
论文： https://arxiv.org/abs/2310.04691
github: https://github.com/DRSY/EMO

NEFTune --> 针对数据集中有大量重复信息有效，极大程度避免对重复数据的过拟合
在 LLM 的预训练或后训练阶段非常有效，但是在本场比赛几乎无影响，只要提前对数据做去重清洗即可。
论文：https://arxiv.org/abs/2310.05914
github: https://github.com/neelsjain/NEFTune

label_smoothing --> 针对 1B 以下的小模型有效，随着模型参数上升，拟合能力和泛化能力变强，额外的人工处理反而降低模型表现能力
论文：https://arxiv.org/abs/1906.02629
扩展资料：https://spaces.ac.cn/archives/9098

# 额外细节
1. SFT 只需要 LoRA 微调 q, k, v三种模块
2. 对于分类问题还需要增加微调 o 和 gate 两个模块
![image-1](D:\xgswj\求职\简历-2024-10-24\image\image-5.png)
3. 不冻结参数会加剧模型的灾难性遗忘，但同时也能加强模型在下游任务的表现。为哪些层使用LoRA和不冻结哪些层参数是一样的
4. LoRA能极大缓解模型的灾难性遗忘问题
5. 数据格式不需要保持和预训练相同，调整q, k, v三个模型时模型几乎瞬间就可以适应新的格式
6. 学习率和批次大小的比例不变情况下，模型性能不会相差太远
7. 对数据的padding是个复杂的问题，取决于模型是否有attention mask。大部分生成式模型在预训练阶段是没有padding的，需要用\<eos>表示。有的以某一个特殊token来表示填充，如\<PAD>，每个厂商有自己的标准，需要查看每个模型的说明来设置。
8. bfloat16, float16, float32
bfloat是比较先进的，其表示范围与fp32相同但精度弱于fp16, 对神经网络来说，精度并不那么重要，更低的精度一定程度起到了正则化的效果，反而让模型泛化性更强了。
9. 神经网络的 bias 参数一般设为 False 比较好，否则容易导致过拟合
![image-1](D:\xgswj\求职\简历-2024-10-24\image\image-6.png)
10. 加速注意力计算可以使用两种方法：缩放点积注意力和flash attention，后者加速更明显，且没有效果损耗，其是完全基于优化IO和计算顺序来做到的。
解说：https://zhuanlan.zhihu.com/p/639228219?s_r=0
github: https://github.com/Dao-AILab/flash-attention

# 可优化点
1. more data: 对 1M 数据打伪标签并训练
2. EMA (代码内已有其LoRA实现，独家一份) --> 单模内集成多个模型权重
3. Rdrop 利用对比学习降低 dropout 的影响
4. 使用8位量化，推理速度更快，可以在线上以3072的长度推理（我只能开到1736）
5. 增加两个预测头，预测response_a和response_b是由哪两个模型给出的。 --> 为模型提供更多参考信息
